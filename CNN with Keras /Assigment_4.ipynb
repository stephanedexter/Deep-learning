{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43be8d4a",
   "metadata": {},
   "source": [
    "## Part A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65371c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24835fcd-9fc0-4afb-8c57-cf7ae7b94be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset\n",
    "data_set = pd.read_csv('concrete_data.csv')\n",
    "data_set.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae139364-6388-4d6c-b3bf-4399e1c526c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41465fef-c8c0-45c3-b437-c2efb5924cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels and target\n",
    "cols = data_set.columns\n",
    "features = data_set[cols[cols!= 'Strength']]\n",
    "target = data_set['Strength']\n",
    "# target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938a62be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "def create_model():\n",
    "    model  = Sequential()\n",
    "    model.add(Dense(10, activation = 'relu', input_shape=(features.shape[1],)))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer = 'adam', loss = 'mse')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00030cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with 50 epochs for 50 loops\n",
    "MSE = []\n",
    "for i in range(50):\n",
    "    model = create_model()\n",
    "    # data for train and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.3, random_state=i)\n",
    "    model.fit(X_train, y_train, epochs = 50, verbose=0)\n",
    "    # evaluate the model\n",
    "    y_predict = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_predict)\n",
    "    MSE.append(mse)\n",
    "# mean and std\n",
    "mse_mean = np.mean(MSE)\n",
    "mse_std = np.std(MSE)\n",
    "\n",
    "# print \n",
    "print(f\"Mean: {mse_mean:.4f}\")\n",
    "print(f\"M_sdt : {mse_std}\")\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46034e7-2670-4829-a1d6-b4090a654e0b",
   "metadata": {},
   "source": [
    "## Part B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0688cd2a-f65d-449e-89f8-ad2efd9fc8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normaliw the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "features_normalized = scaler.fit_transform(features)\n",
    "\n",
    "# Create the model\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, activation='relu', input_shape=(features_normalized.shape[1],))) \n",
    "    model.add(Dense(1))  \n",
    "    model.compile(optimizer= 'adam', loss='mean_squared_error')  \n",
    "    return model\n",
    "\n",
    "# loop \n",
    "mse_list_normalized = []\n",
    "for i in range(50):\n",
    "    # Divided data in two parts: data train and data test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features_normalized, target, test_size=0.3, random_state=i)\n",
    "    \n",
    "    # Train the model\n",
    "    model = create_model()\n",
    "    model.fit(X_train, y_train, epochs=50, verbose=0)\n",
    "    \n",
    "    # prediction\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # MSE\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "    # add mse in the list\n",
    "    mse_list_normalized.append(mse)\n",
    "\n",
    "# Mean and Std\n",
    "mse_mean_normalized = np.mean(mse_list_normalized)\n",
    "mse_std_normalized = np.std(mse_list_normalized)\n",
    "\n",
    "# print \n",
    "print(f\"Mean_normalized: {mse_mean_normalized:.4f}\")\n",
    "print(f\"M_sdt_normalized : {mse_std_normalized:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913e5134-553e-4bee-9f33-6f117ffa934d",
   "metadata": {},
   "source": [
    "mse_mean_normalized is higher than mse_mean: normalization might not improve the model's performance, that can be explain by the raw data was already on a similar scale or  normalization disrupted some inherent structure in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f8a5a8-a9e2-4ee3-b127-62408833bcb5",
   "metadata": {},
   "source": [
    "## Part C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479f7119-17ea-4f29-bff9-7b6c5ab97f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normaliw the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "features_normalized = scaler.fit_transform(features)\n",
    "\n",
    "# Create the model\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, activation='relu', input_shape=(features_normalized.shape[1],)))  \n",
    "    model.add(Dense(1))  \n",
    "    model.compile(optimizer= 'adam', loss='mean_squared_error') \n",
    "    return model\n",
    "\n",
    "# loop with 100 epochs\n",
    "mse_list_normalized = []\n",
    "for i in range(50):\n",
    "    # Divided data in two parts: data train and data test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features_normalized, target, test_size=0.3, random_state=i)\n",
    "    \n",
    "    # Train the model\n",
    "    model = create_model()\n",
    "    model.fit(X_train, y_train, epochs=100, verbose=0)\n",
    "    \n",
    "    # Prediction\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # MSE\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "    # add mse in the list\n",
    "    mse_list_normalized.append(mse)\n",
    "\n",
    "# Mean and Std\n",
    "mse_mean_normalized = np.mean(mse_list_normalized)\n",
    "mse_std_normalized = np.std(mse_list_normalized)\n",
    "\n",
    "# print \n",
    "print(f\"Mean_normalized: {mse_mean_normalized:.4f}\")\n",
    "print(f\"M_sdt_normalized : {mse_std_normalized:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fcbb42-3f80-4832-a578-3fc581582e13",
   "metadata": {},
   "source": [
    "mse_mean_normalized decreased copared to that from the Part B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1064d86-e3f1-4688-92ad-bd5a4cfeb58f",
   "metadata": {},
   "source": [
    "## Part D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7884b1fe-493e-4640-a779-bc138f73b066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normaliw the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "features_normalized = scaler.fit_transform(features)\n",
    "\n",
    "# Create the model\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, activation='relu', input_shape=(features_normalized.shape[1],))) \n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(1))  \n",
    "    model.compile(optimizer= 'adam', loss='mean_squared_error')  \n",
    "    return model\n",
    "\n",
    "# loop with 100 epochs\n",
    "mse_list_normalized = []\n",
    "for i in range(50):\n",
    "    # Divided data in two parts: data train and data test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features_normalized, target, test_size=0.3, random_state=i)\n",
    "    \n",
    "    # Train the model\n",
    "    model = create_model()\n",
    "    model.fit(X_train, y_train, epochs=100, verbose=0)\n",
    "    \n",
    "    # Prediction\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # MSE\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "    # add mse in the list\n",
    "    mse_list_normalized.append(mse)\n",
    "\n",
    "# Mean and Std\n",
    "mse_mean_normalized = np.mean(mse_list_normalized)\n",
    "mse_std_normalized = np.std(mse_list_normalized)\n",
    "\n",
    "# print \n",
    "print(f\"Mean_normalized: {mse_mean_normalized:.4f}\")\n",
    "print(f\"M_sdt_normalized : {mse_std_normalized:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a4ce97-365a-48b7-bd22-d02317a26556",
   "metadata": {},
   "source": [
    "The mse_mean_normalized decreased compared to that from Part B and Part C. This means that increasing the number of hidden layers had a positive impact on both the training and test performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
